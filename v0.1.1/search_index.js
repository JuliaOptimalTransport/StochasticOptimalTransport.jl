var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = StochasticOptimalTransport","category":"page"},{"location":"#StochasticOptimalTransport","page":"Home","title":"StochasticOptimalTransport","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [StochasticOptimalTransport]","category":"page"},{"location":"#StochasticOptimalTransport.DiscreteMeasure-Tuple{AbstractVector{T} where T, AbstractVector{T} where T}","page":"Home","title":"StochasticOptimalTransport.DiscreteMeasure","text":"DiscreteMeasure(xs::AbstractVector, ps::AbstractVector)\n\nConstruct a discrete measure with support xs and corresponding weights ps.\n\n\n\n\n\n","category":"method"},{"location":"#StochasticOptimalTransport.ctransform-Tuple{Any, AbstractVector{T} where T, Any, StochasticOptimalTransport.DiscreteMeasure, Nothing}","page":"Home","title":"StochasticOptimalTransport.ctransform","text":"ctransform(c, v, x, ν, ε)\n\nCompute the c-transform\n\nv^cε(x) = begincases\n- ε logbigg(int expBig(fracv_y - c(x y)εBig)  ν(mathrmdy)bigg)  textif  ε  0\nmin_y c(x y) - v_y  textotherwise\nendcases\n\n\n\n\n\n","category":"method"},{"location":"#StochasticOptimalTransport.wasserstein-Tuple","page":"Home","title":"StochasticOptimalTransport.wasserstein","text":"wasserstein([rng, ]c, μ, ν[, ε; kwargs...])\n\nEstimate the (entropic regularization of the) Wasserstein distance\n\nW_ε(μ ν) = min_π  Π(μν) int c(x y) π(mathrmd(xy)) +\nε mathrmKL(π  μ  ν)\n\nwith respect to cost function c using stochastic optimization.\n\nIf both measures μ and ν are DiscreteMeasures, then the Wasserstein distance is approximated with stochastic averaged gradient descent (SAG). The convergence rate of SAG is O(1  k), where k is the number of iterations, and hence converges faster than stochastic gradient descent (SGD) at the price of increased memory consumption.\n\nIf only one of the measures μ and ν is a DiscreteMeasure, then the Wasserstein distance is approximated with stochastic gradient descent with averaging (SGA). In this case, it is required that samples of the non-discrete measure λ can be obtained with rand(rng, λ). The convergence rate of SGA is O(1k), where k is the number of iterations.\n\nIf ε is nothing (the default), then the unregularized Wasserstein distance is approximated. Otherwise, the entropic regularization with ε > 0 is estimated.\n\nThe SAG algorithm uses a constant step size, whereas the SGA algorithm uses the step size schedule\n\n    τᵢ = fracτ₁1 + sqrt(i - 1)  w\n\nfor the ith iteration, where τ₁ corresponds to the initial step size and w indicates the number of iterations serving as a warm-up phase.\n\nKeyword arguments\n\nmaxiters::Int = 10_000: maximum number of gradient steps\nstepsize = 1: constant stepsize of SAG or initial step size τ₁ of SGA\nwarmup_phase = 1: warm-up phase w of SGA\natol = 0: absolute tolerance\nrtol = iszero(atol) ? typeof(float(atol))(1 // 10_000) : 0: relative tolerance\nmontecarlo_samples = 10_000: Number of Monte Carlo samples from μ for approximating an expectation with respect to μ in the semi-discrete optimal transport problem\n\nReferences\n\nGenevay et al. (2016). Stochastic Optimization for Large-Scale Optimal Transport. Advances in Neural Information Processing Systems (NIPS 2016), 29:3440-3448.\n\nPeyré, Gabriel, & Marco Cuturi (2019). Computational Optimal Transport. Foundations and Trends in Machine Learning, 11(5-6):355-607.\n\n\n\n\n\n","category":"method"}]
}
